#!/usr/bin/env python3
"""
åˆå¹¶ Brain LLM çš„ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import os

# è·¯å¾„é…ç½®
BASE_MODEL_PATH = "../models/Qwen/Qwen2.5-0.5B-Instruct"
LORA_PATH = "output/Qwen2.5-0.5B-Brain/lora/sft"
OUTPUT_PATH = "output/qwen2.5-0.5b-brain-merged"

def main():
    print("=" * 60)
    print("ğŸ§  åˆå¹¶ Brain LLM LoRA æƒé‡")
    print("=" * 60)
    
    # åŠ è½½åŸºåº§æ¨¡å‹
    print("\n1. åŠ è½½åŸºåº§æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16
    )
    
    # åŠ è½½ LoRA æƒé‡
    print("\n2. åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    
    # åˆå¹¶æƒé‡
    print("\n3. åˆå¹¶æƒé‡...")
    model = model.merge_and_unload()
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    print(f"\n4. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ° {OUTPUT_PATH}...")
    os.makedirs(OUTPUT_PATH, exist_ok=True)
    model.save_pretrained(OUTPUT_PATH)
    tokenizer.save_pretrained(OUTPUT_PATH)
    
    print("\n" + "=" * 60)
    print("âœ… åˆå¹¶å®Œæˆ!")
    print(f"   è¾“å‡ºè·¯å¾„: {OUTPUT_PATH}")
    print("=" * 60)
    print("\nä¸‹ä¸€æ­¥: è¿è¡Œ convert_brain_to_gguf.sh è¿›è¡Œé‡åŒ–")


if __name__ == "__main__":
    main()

