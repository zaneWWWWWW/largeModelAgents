#!/bin/bash
# Brain LLM é‡åŒ–è„šæœ¬
# å°†åˆå¹¶åçš„æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼

set -e

INPUT_DIR="output/qwen2.5-0.5b-brain-merged"
OUTPUT_DIR="output/brain-gguf"
LLAMA_CPP_DIR="llama.cpp"

echo "=========================================="
echo "ğŸ”§ å°† Brain LLM è½¬æ¢ä¸º GGUF æ ¼å¼"
echo "=========================================="

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p $OUTPUT_DIR

# æ£€æŸ¥ llama.cpp ç›®å½•
if [ ! -d "$LLAMA_CPP_DIR" ]; then
    echo "æ­£åœ¨è§£å‹ llama.cpp..."
    if [ -f "../psychological-assessment/llama.cpp.zip" ]; then
        unzip -q ../psychological-assessment/llama.cpp.zip -d .
    elif [ -f "llama.cpp.zip" ]; then
        unzip -q llama.cpp.zip -d .
    else
        echo "é”™è¯¯: æ‰¾ä¸åˆ° llama.cpp.zip æ–‡ä»¶"
        exit 1
    fi
fi

cd $LLAMA_CPP_DIR

# è½¬æ¢ä¸º GGUF æ ¼å¼ (FP16)
echo ""
echo "1. è½¬æ¢ä¸º GGUF (FP16)..."
python convert_hf_to_gguf.py ../$INPUT_DIR --outfile ../$OUTPUT_DIR/brain-fp16.gguf --outtype f16

# é‡åŒ–ä¸º Q4_K_M (æ¨èçš„å¹³è¡¡é‡åŒ–)
echo ""
echo "2. é‡åŒ–ä¸º Q4_K_M..."
./build/bin/llama-quantize ../$OUTPUT_DIR/brain-fp16.gguf ../$OUTPUT_DIR/brain-q4_k_m.gguf Q4_K_M

# é‡åŒ–ä¸º Q8_0 (æ›´é«˜è´¨é‡)
echo ""
echo "3. é‡åŒ–ä¸º Q8_0..."
./build/bin/llama-quantize ../$OUTPUT_DIR/brain-fp16.gguf ../$OUTPUT_DIR/brain-q8_0.gguf Q8_0

cd ..

echo ""
echo "=========================================="
echo "âœ… GGUF è½¬æ¢å®Œæˆ!"
echo "=========================================="
echo "è¾“å‡ºæ–‡ä»¶:"
echo "  - $OUTPUT_DIR/brain-fp16.gguf    (FP16, çº¦ 1GB)"
echo "  - $OUTPUT_DIR/brain-q4_k_m.gguf  (Q4_K_M, çº¦ 350MB, æ¨è)"
echo "  - $OUTPUT_DIR/brain-q8_0.gguf    (Q8_0, çº¦ 530MB)"
echo ""
echo "ğŸ¤– Android éƒ¨ç½²æ¨èä½¿ç”¨ brain-q4_k_m.gguf"
echo "=========================================="

