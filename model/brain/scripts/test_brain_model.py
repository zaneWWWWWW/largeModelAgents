#!/usr/bin/env python3
"""
Brain LLM æµ‹è¯•è„šæœ¬
æµ‹è¯•å·¥å…·è·¯ç”±å†³ç­–èƒ½åŠ›
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import json
import re

# è·¯å¾„é…ç½®
BASE_MODEL_PATH = "../models/Qwen/Qwen2.5-0.5B-Instruct"
LORA_PATH = "output/Qwen2.5-0.5B-Brain/lora/sft"

# System Prompt (ä¸è®­ç»ƒæ•°æ®ä¸€è‡´)
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å¿ƒç†å’¨è¯¢åŠ©æ‰‹çš„å†³ç­–å¤§è„‘(Brain LLM)ã€‚ä½ çš„èŒè´£æ˜¯æ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­éœ€è¦è°ƒç”¨å“ªäº›å·¥å…·ã€‚

ğŸ”§ å¯ç”¨å·¥å…·ï¼š

1. local_chatï¼ˆå¿…é¡»è°ƒç”¨ï¼‰
   - æè¿°ï¼šç”Ÿæˆå¯¹è¯å›å¤ï¼Œæä¾›æƒ…æ„Ÿæ”¯æŒå’Œå»ºè®®
   - å‚æ•°ï¼š{"user_input": "ç”¨æˆ·çš„åŸå§‹è¾“å…¥"}
   - è¯´æ˜ï¼šæ¯æ¬¡éƒ½å¿…é¡»è°ƒç”¨ï¼Œæ˜¯ä¸ç”¨æˆ·äº¤äº’çš„ä¸»è¦æ–¹å¼

2. psychological_assessmentï¼ˆæŒ‰éœ€è°ƒç”¨ï¼‰
   - æè¿°ï¼šå½“ç”¨æˆ·è¡¨è¾¾ä¸¥é‡å¿ƒç†å›°æ‰°æ—¶ï¼Œè¿›è¡Œä¸“ä¸šè¯„ä¼°
   - å‚æ•°ï¼š{"trigger_reason": "è§¦å‘è¯„ä¼°çš„å…·ä½“åŸå› "}
   - è§¦å‘æ¡ä»¶ï¼šç„¦è™‘ã€æŠ‘éƒã€å¤±çœ ï¼ˆæŒç»­æ€§ï¼‰ã€è‡ªæ€/è‡ªä¼¤å€¾å‘ã€å¼ºè¿«è¡Œä¸ºç­‰

3. memory_queryï¼ˆæŒ‰éœ€è°ƒç”¨ï¼‰
   - æè¿°ï¼šæŸ¥è¯¢å†å²å¯¹è¯è®°å½•
   - å‚æ•°ï¼š{"query": "æŸ¥è¯¢å…³é”®è¯"}
   - è§¦å‘æ¡ä»¶ï¼šç”¨æˆ·æ˜ç¡®è¯¢é—®å†å²å¯¹è¯å†…å®¹

ğŸ“‹ è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
```json
{
  "tools": [
    {"name": "local_chat", "parameters": {"user_input": "..."}},
    {"name": "psychological_assessment", "parameters": {"trigger_reason": "..."}}
  ]
}
```

âš ï¸ é‡è¦è§„åˆ™ï¼š
1. tools æ•°ç»„ä¸­å¿…é¡»åŒ…å« local_chat
2. æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–å·¥å…·ï¼ˆ0-2ä¸ªï¼‰
3. åªè¾“å‡º JSONï¼Œä¸è¦é¢å¤–è§£é‡Š"""


def extract_json(text):
    """ä»è¾“å‡ºä¸­æå– JSON"""
    # å°è¯•ä» markdown ä»£ç å—ä¸­æå–
    match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
    if match:
        return match.group(1).strip()
    # å°è¯•ç›´æ¥åŒ¹é… JSON
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        return match.group(0)
    return None


def load_model():
    """åŠ è½½æ¨¡å‹"""
    print("æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16
    )
    
    print("æ­£åœ¨åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    
    return model, tokenizer


def brain_decide(model, tokenizer, user_input):
    """è°ƒç”¨ Brain LLM è¿›è¡Œå†³ç­–"""
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_input}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=128,
            temperature=0.1,  # ä½æ¸©åº¦ä¿è¯è¾“å‡ºç¨³å®š
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # åªä¿ç•™æ–°ç”Ÿæˆçš„ token
    generated_ids = generated_ids[0][len(model_inputs.input_ids[0]):]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return response


def test_brain():
    """æµ‹è¯• Brain LLM"""
    model, tokenizer = load_model()
    
    # æµ‹è¯•ç”¨ä¾‹ - æœŸæœ›å·¥å…·åˆ—è¡¨
    test_cases = [
        # ä»… local_chat åœºæ™¯
        ("ä½ å¥½", ["local_chat"]),
        ("ä»Šå¤©å¤©æ°”çœŸä¸é”™", ["local_chat"]),
        ("æœ€è¿‘æœ‰ç‚¹å°çƒ¦æ¼", ["local_chat"]),
        ("æœ‰ä»€ä¹ˆå¥½çœ‹çš„ä¹¦æ¨èå—", ["local_chat"]),
        ("è°¢è°¢ä½ çš„å»ºè®®", ["local_chat"]),
        
        # local_chat + psychological_assessment åœºæ™¯
        ("æˆ‘æœ€è¿‘æ€»æ˜¯ç¡ä¸ç€ï¼Œå·²ç»ä¸€å‘¨äº†", ["local_chat", "psychological_assessment"]),
        ("æˆ‘æ„Ÿè§‰æ´»ç€æ²¡ä»€ä¹ˆæ„æ€", ["local_chat", "psychological_assessment"]),
        ("æˆ‘æ¯å¤©éƒ½å¾ˆç„¦è™‘ï¼Œå¿ƒæ…Œ", ["local_chat", "psychological_assessment"]),
        ("æˆ‘æƒ³è‡ªæ€", ["local_chat", "psychological_assessment"]),
        ("æˆ‘æ§åˆ¶ä¸ä½åœ°å“­æ³£", ["local_chat", "psychological_assessment"]),
        
        # local_chat + memory_query åœºæ™¯
        ("ä¸Šæ¬¡æˆ‘ä»¬èŠäº†ä»€ä¹ˆ", ["local_chat", "memory_query"]),
        ("æˆ‘ä¹‹å‰è¯´è¿‡ä»€ä¹ˆå—", ["local_chat", "memory_query"]),
        ("ä½ è¿˜è®°å¾—æˆ‘ä¹‹å‰æåˆ°çš„é—®é¢˜å—", ["local_chat", "memory_query"]),
        ("å¸®æˆ‘å›å¿†ä¸€ä¸‹ä¸Šå‘¨çš„å¯¹è¯", ["local_chat", "memory_query"]),
    ]
    
    print("\n" + "=" * 70)
    print("ğŸ§  Brain LLM å·¥å…·è·¯ç”±å†³ç­–æµ‹è¯•")
    print("=" * 70)
    
    correct = 0
    total = len(test_cases)
    
    for user_input, expected_tools in test_cases:
        print(f"\nğŸ“¥ ç”¨æˆ·è¾“å…¥: {user_input}")
        print(f"   æœŸæœ›å·¥å…·: {expected_tools}")
        
        response = brain_decide(model, tokenizer, user_input)
        print(f"   æ¨¡å‹è¾“å‡º: {response.strip()}")
        
        # è§£æ JSON
        json_str = extract_json(response)
        if json_str:
            try:
                result = json.loads(json_str)
                tools_list = result.get("tools", [])
                actual_tools = [t.get("name", "") for t in tools_list if isinstance(t, dict)]
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…
                if set(actual_tools) == set(expected_tools):
                    print(f"   âœ… æ­£ç¡®! è°ƒç”¨å·¥å…·: {actual_tools}")
                    correct += 1
                else:
                    print(f"   âŒ é”™è¯¯! å®é™…è°ƒç”¨: {actual_tools}, æœŸæœ›: {expected_tools}")
            except json.JSONDecodeError:
                print(f"   âŒ JSON è§£æå¤±è´¥")
        else:
            print(f"   âŒ æœªæ‰¾åˆ° JSON è¾“å‡º")
    
    print("\n" + "=" * 70)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {correct}/{total} ({correct/total*100:.1f}%)")
    print("=" * 70)


if __name__ == "__main__":
    test_brain()

