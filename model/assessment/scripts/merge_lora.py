#!/usr/bin/env python3
"""
åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹
"""
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import os

# è·¯å¾„é…ç½®
base_model_path = "models/Qwen/Qwen2___5-0___5B-Instruct"
lora_path = "saves/Qwen2.5-0.5B/lora/sft"
output_path = "output/qwen2.5-0.5b-merged"

print("="*60)
print("å¼€å§‹åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹")
print("="*60)

print(f"\nğŸ“ åŸºåº§æ¨¡å‹è·¯å¾„: {base_model_path}")
print(f"ğŸ“ LoRA æƒé‡è·¯å¾„: {lora_path}")
print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(output_path, exist_ok=True)

print("\nâ³ æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    device_map="cpu",  # ä½¿ç”¨ CPU ä»¥èŠ‚çœæ˜¾å­˜
    trust_remote_code=True
)

print("â³ æ­£åœ¨åŠ è½½ LoRA æƒé‡...")
model = PeftModel.from_pretrained(model, lora_path)

print("â³ æ­£åœ¨åˆå¹¶æƒé‡...")
model = model.merge_and_unload()

print("â³ æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
model.save_pretrained(output_path, max_shard_size="2GB")
tokenizer.save_pretrained(output_path)

print("\nâœ… æ¨¡å‹åˆå¹¶å®Œæˆï¼")
print(f"ğŸ“ åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨: {output_path}")



