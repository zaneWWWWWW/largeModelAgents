CUDA_VISIBLE_DEVICES=0
Mon Nov 24 16:20:39 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA RTX A6000               On  | 00000000:AF:00.0 Off |                  Off |
| 30%   32C    P8              24W / 300W |      3MiB / 49140MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
torch: 2.5.1+cu121 cuda: True
[INFO|2025-11-24 16:20:56] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16
[INFO|2025-11-24 16:20:57] llamafactory.data.loader:143 >> Loading dataset sft_student_mental.jsonl...
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14880, 100345, 105051, 43815, 102086, 116743, 105151, 90395, 91680, 66017, 5370, 5122, 450, 4011, 8274, 7, 15, 12, 18, 701, 18056, 8274, 7, 15, 12, 18, 701, 5214, 10933, 1445, 603, 91, 27051, 292, 25880, 91, 721, 1523, 2178, 91, 55151, 763, 701, 5458, 16031, 673, 10405, 7, 15, 12, 24, 8, 8997, 872, 25, 88940, 225, 21887, 100703, 99235, 3837, 108386, 3837, 35946, 104044, 99879, 35946, 104858, 87267, 113783, 34187, 3837, 43288, 104029, 99491, 104573, 1773, 35946, 114606, 94432, 872, 25, 49434, 239, 33108, 35946, 104858, 20412, 99562, 101181, 3837, 97639, 104080, 99461, 109745, 34187, 3837, 104133, 20, 104252, 99657, 1773, 100543, 24562, 3837, 111003, 42411, 87267, 113783, 34187, 8997, 872, 25, 49434, 239, 106546, 100648, 58405, 69249, 33108, 46944, 57750, 9370, 101194, 105610, 33108, 87805, 1773, 42411, 104136, 36587, 99650, 100009, 99614, 3837, 105984, 104083, 100698, 8997, 872, 25, 49434, 239, 79478, 99601, 106822, 105919, 104432, 1773, 35946, 104014, 106118, 85336, 100111, 100648, 58405, 3837, 97639, 103977, 99519, 111553, 116784, 8997, 872, 25, 49434, 239, 109623, 35946, 107999, 101901, 111852, 42411, 3837, 70361, 100641, 102497, 1773, 110146, 104359, 104899, 103952, 104617, 33108, 101064, 8997, 872, 25, 49434, 239, 99880, 103952, 104617, 100006, 101954, 3837, 101064, 117809, 1773, 101553, 99880, 99519, 111553, 99564, 26939, 103952, 100145, 8997, 872, 25, 49434, 239, 101045, 99165, 104409, 1773, 102389, 3837, 106922, 99242, 42411, 3837, 104359, 104899, 103952, 104617, 24968, 104702, 3837, 35946, 104083, 102827, 111553, 3837, 70361, 100641, 102497, 8997, 872, 25, 18137, 224, 96, 110693, 99730, 109031, 94432, 872, 25, 4891, 98, 121, 9370, 3837, 105351, 101066, 85336, 104482, 1773, 102570, 103929, 100364, 1773, 151645, 198, 151644, 77091, 198, 4913, 450, 4011, 8274, 788, 220, 16, 11, 330, 276, 16308, 8274, 788, 220, 17, 11, 330, 80943, 10933, 788, 330, 6697, 497, 330, 12038, 16031, 673, 10405, 788, 220, 21, 92, 151645, 198]
inputs:
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
请根据对话内容评估四项标签，并只输出JSON：depression_level(0-3), anxiety_level(0-3), risk_flag(none|suicidal|self_harm|violence), student_distress_score(0-9)。
user: 心理咨询师，你好，我最近发现我老公可能出轨了，这让我非常痛苦。我该怎么办？
user: 我和我老公是大学同学，我们在一起已经十几年了，有一个5岁的孩子。半年前，我发现他可能出轨了。
user: 我发现了他的手机里和一个女的有很多短信和电话。他解释说他们只是朋友，但我很难相信。
user: 我们现在相处得很紧张。我总是忍不住去查看他的手机，我们也会因为这件事情争吵。
user: 我想知道我该如何才能原谅他，重新建立信任。我真的不想失去我们的婚姻和家庭。
user: 我希望我们的婚姻能够幸福，家庭和睦。我不希望因为这件事情影响到我们的关系。
user: 我其实很矛盾。一方面，我很爱他，不想失去我们的婚姻；另一方面，我很难忘记这件事情，重新建立信任。
user: 那我们现在应该怎么做？
user: 好的，我会努力去尝试。谢谢你的帮助。<|im_end|>
<|im_start|>assistant
{"depression_level": 1, "anxiety_level": 2, "risk_flag": "none", "student_distress_score": 6}<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 450, 4011, 8274, 788, 220, 16, 11, 330, 276, 16308, 8274, 788, 220, 17, 11, 330, 80943, 10933, 788, 330, 6697, 497, 330, 12038, 16031, 673, 10405, 788, 220, 21, 92, 151645, 198]
labels:
{"depression_level": 1, "anxiety_level": 2, "risk_flag": "none", "student_distress_score": 6}<|im_end|>

[INFO|2025-11-24 16:21:02] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-11-24 16:21:04] llamafactory.model.model_utils.checkpointing:143 >> Upcasting layernorm weights in float32.
[INFO|2025-11-24 16:21:04] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-11-24 16:21:04] llamafactory.model.model_utils.checkpointing:143 >> Upcasting lm_head outputs in float32.
[INFO|2025-11-24 16:21:04] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-11-24 16:21:04] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-11-24 16:21:04] llamafactory.model.adapter:143 >> Fine-tuning method: DoRA
[INFO|2025-11-24 16:21:04] llamafactory.model.model_utils.misc:143 >> Found linear modules: v_proj,k_proj,gate_proj,up_proj,down_proj,o_proj,q_proj
[INFO|2025-11-24 16:21:05] llamafactory.model.loader:143 >> trainable params: 7,827,456 || all params: 471,815,168 || trainable%: 1.6590
{'loss': 0.5228, 'grad_norm': 2.6342227458953857, 'learning_rate': 5.232558139534884e-06, 'epoch': 0.02}
{'loss': 0.4027, 'grad_norm': 1.6951842308044434, 'learning_rate': 1.1046511627906977e-05, 'epoch': 0.03}
{'loss': 0.233, 'grad_norm': 0.4808723032474518, 'learning_rate': 1.686046511627907e-05, 'epoch': 0.05}
{'loss': 0.1678, 'grad_norm': 0.4144045412540436, 'learning_rate': 2.2674418604651163e-05, 'epoch': 0.07}
{'loss': 0.1503, 'grad_norm': 0.3231678605079651, 'learning_rate': 2.848837209302326e-05, 'epoch': 0.09}
{'loss': 0.1415, 'grad_norm': 0.35949158668518066, 'learning_rate': 3.430232558139535e-05, 'epoch': 0.1}
{'loss': 0.1371, 'grad_norm': 0.4087023138999939, 'learning_rate': 4.0116279069767444e-05, 'epoch': 0.12}
{'loss': 0.1236, 'grad_norm': 0.4660230576992035, 'learning_rate': 4.593023255813954e-05, 'epoch': 0.14}
{'loss': 0.1195, 'grad_norm': 0.5848084688186646, 'learning_rate': 5.1744186046511636e-05, 'epoch': 0.16}
{'loss': 0.1244, 'grad_norm': 1.551065444946289, 'learning_rate': 5.755813953488373e-05, 'epoch': 0.17}
{'loss': 0.1144, 'grad_norm': 0.6121097207069397, 'learning_rate': 6.337209302325582e-05, 'epoch': 0.19}
{'loss': 0.1138, 'grad_norm': 0.5920374989509583, 'learning_rate': 6.918604651162791e-05, 'epoch': 0.21}
{'loss': 0.1105, 'grad_norm': 0.5796317458152771, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.23}
{'loss': 0.1088, 'grad_norm': 0.3686249852180481, 'learning_rate': 8.081395348837209e-05, 'epoch': 0.24}
{'loss': 0.106, 'grad_norm': 0.45001527667045593, 'learning_rate': 8.662790697674419e-05, 'epoch': 0.26}
{'loss': 0.1061, 'grad_norm': 0.49456679821014404, 'learning_rate': 9.244186046511628e-05, 'epoch': 0.28}
{'loss': 0.1054, 'grad_norm': 0.5320540070533752, 'learning_rate': 9.825581395348838e-05, 'epoch': 0.3}
{'loss': 0.1034, 'grad_norm': 0.6110552549362183, 'learning_rate': 9.999492852953918e-05, 'epoch': 0.31}
{'loss': 0.1001, 'grad_norm': 0.27508652210235596, 'learning_rate': 9.997009115083239e-05, 'epoch': 0.33}
{'loss': 0.0911, 'grad_norm': 0.49575692415237427, 'learning_rate': 9.992456663888425e-05, 'epoch': 0.35}
{'loss': 0.0911, 'grad_norm': 0.28320929408073425, 'learning_rate': 9.985837384040138e-05, 'epoch': 0.37}
{'loss': 0.0919, 'grad_norm': 0.4122617244720459, 'learning_rate': 9.97715401585605e-05, 'epoch': 0.38}
{'loss': 0.0957, 'grad_norm': 0.30446362495422363, 'learning_rate': 9.966410154166403e-05, 'epoch': 0.4}
{'loss': 0.0905, 'grad_norm': 0.25750648975372314, 'learning_rate': 9.953610246825765e-05, 'epoch': 0.42}
{'loss': 0.0945, 'grad_norm': 0.30756813287734985, 'learning_rate': 9.938759592871671e-05, 'epoch': 0.44}
{'loss': 0.0901, 'grad_norm': 0.3403764069080353, 'learning_rate': 9.921864340330871e-05, 'epoch': 0.45}
{'loss': 0.0889, 'grad_norm': 0.3465958833694458, 'learning_rate': 9.902931483674105e-05, 'epoch': 0.47}
{'loss': 0.0883, 'grad_norm': 0.7710466384887695, 'learning_rate': 9.881968860920463e-05, 'epoch': 0.49}
{'loss': 0.0864, 'grad_norm': 0.42243829369544983, 'learning_rate': 9.858985150392515e-05, 'epoch': 0.51}
{'loss': 0.0904, 'grad_norm': 0.1750710904598236, 'learning_rate': 9.833989867123577e-05, 'epoch': 0.52}
{'loss': 0.0872, 'grad_norm': 0.3925444483757019, 'learning_rate': 9.806993358918578e-05, 'epoch': 0.54}
{'loss': 0.0874, 'grad_norm': 0.46770793199539185, 'learning_rate': 9.778006802070162e-05, 'epoch': 0.56}
{'loss': 0.0864, 'grad_norm': 0.3475155830383301, 'learning_rate': 9.74704219673182e-05, 'epoch': 0.58}
{'loss': 0.0871, 'grad_norm': 0.2406759411096573, 'learning_rate': 9.71411236194994e-05, 'epoch': 0.59}
{'loss': 0.0824, 'grad_norm': 0.3000398874282837, 'learning_rate': 9.679230930356841e-05, 'epoch': 0.61}
{'loss': 0.0812, 'grad_norm': 0.3061961233615875, 'learning_rate': 9.642412342527006e-05, 'epoch': 0.63}
{'loss': 0.0842, 'grad_norm': 0.45840272307395935, 'learning_rate': 9.603671840998811e-05, 'epoch': 0.65}
{'loss': 0.09, 'grad_norm': 0.33059102296829224, 'learning_rate': 9.563025463964275e-05, 'epoch': 0.66}
{'loss': 0.09, 'grad_norm': 0.24448150396347046, 'learning_rate': 9.520490038629394e-05, 'epoch': 0.68}
{'loss': 0.0793, 'grad_norm': 0.4601301848888397, 'learning_rate': 9.476083174247845e-05, 'epoch': 0.7}
{'loss': 0.0778, 'grad_norm': 0.2906897962093353, 'learning_rate': 9.429823254830929e-05, 'epoch': 0.72}
{'loss': 0.0768, 'grad_norm': 0.35013410449028015, 'learning_rate': 9.381729431536758e-05, 'epoch': 0.73}
{'loss': 0.0793, 'grad_norm': 0.2589696943759918, 'learning_rate': 9.331821614741876e-05, 'epoch': 0.75}
{'loss': 0.0814, 'grad_norm': 0.2554416060447693, 'learning_rate': 9.280120465798543e-05, 'epoch': 0.77}
{'loss': 0.087, 'grad_norm': 0.1874115914106369, 'learning_rate': 9.226647388481144e-05, 'epoch': 0.79}
{'loss': 0.0821, 'grad_norm': 0.3026027977466583, 'learning_rate': 9.171424520125227e-05, 'epoch': 0.8}
{'loss': 0.0777, 'grad_norm': 0.2843359410762787, 'learning_rate': 9.11447472246287e-05, 'epoch': 0.82}
{'loss': 0.0817, 'grad_norm': 0.26757439970970154, 'learning_rate': 9.055821572158133e-05, 'epoch': 0.84}
{'loss': 0.0799, 'grad_norm': 0.2570665180683136, 'learning_rate': 8.995489351046562e-05, 'epoch': 0.86}
{'loss': 0.0839, 'grad_norm': 0.428140252828598, 'learning_rate': 8.933503036082733e-05, 'epoch': 0.87}
{'loss': 0.0805, 'grad_norm': 0.29671531915664673, 'learning_rate': 8.86988828900004e-05, 'epoch': 0.89}
{'loss': 0.0777, 'grad_norm': 0.27014756202697754, 'learning_rate': 8.804671445686985e-05, 'epoch': 0.91}
{'loss': 0.0779, 'grad_norm': 0.24565239250659943, 'learning_rate': 8.737879505284378e-05, 'epoch': 0.93}
{'loss': 0.0827, 'grad_norm': 0.4152669906616211, 'learning_rate': 8.66954011900795e-05, 'epoch': 0.94}
{'loss': 0.0803, 'grad_norm': 0.3326442241668701, 'learning_rate': 8.59968157870102e-05, 'epoch': 0.96}
{'loss': 0.0816, 'grad_norm': 0.21888461709022522, 'learning_rate': 8.52833280512195e-05, 'epoch': 0.98}
{'loss': 0.0794, 'grad_norm': 0.35970762372016907, 'learning_rate': 8.455523335971223e-05, 'epoch': 1.0}
{'loss': 0.0787, 'grad_norm': 0.40218546986579895, 'learning_rate': 8.381283313663129e-05, 'epoch': 1.01}
{'loss': 0.076, 'grad_norm': 0.22798562049865723, 'learning_rate': 8.305643472847095e-05, 'epoch': 1.03}
{'loss': 0.0765, 'grad_norm': 0.20808547735214233, 'learning_rate': 8.228635127683836e-05, 'epoch': 1.05}
{'loss': 0.0742, 'grad_norm': 0.2919020354747772, 'learning_rate': 8.150290158881604e-05, 'epoch': 1.07}
{'loss': 0.0782, 'grad_norm': 0.3488544523715973, 'learning_rate': 8.07064100049787e-05, 'epoch': 1.08}
{'loss': 0.0773, 'grad_norm': 0.4179922938346863, 'learning_rate': 7.989720626511947e-05, 'epoch': 1.1}
{'loss': 0.0778, 'grad_norm': 0.23894736170768738, 'learning_rate': 7.907562537174091e-05, 'epoch': 1.12}
{'loss': 0.0761, 'grad_norm': 0.2857138216495514, 'learning_rate': 7.824200745136707e-05, 'epoch': 1.14}
{'loss': 0.0709, 'grad_norm': 0.35721808671951294, 'learning_rate': 7.739669761373444e-05, 'epoch': 1.15}
{'loss': 0.0758, 'grad_norm': 0.2596260905265808, 'learning_rate': 7.654004580891997e-05, 'epoch': 1.17}
{'loss': 0.0777, 'grad_norm': 0.3078050911426544, 'learning_rate': 7.567240668246496e-05, 'epoch': 1.19}
{'loss': 0.0788, 'grad_norm': 0.3487057685852051, 'learning_rate': 7.479413942855544e-05, 'epoch': 1.21}
{'loss': 0.0765, 'grad_norm': 0.22917810082435608, 'learning_rate': 7.390560764131909e-05, 'epoch': 1.22}
{'loss': 0.0722, 'grad_norm': 0.23959755897521973, 'learning_rate': 7.300717916430088e-05, 'epoch': 1.24}
{'loss': 0.0764, 'grad_norm': 0.48387107253074646, 'learning_rate': 7.209922593817941e-05, 'epoch': 1.26}
{'loss': 0.0694, 'grad_norm': 0.25536829233169556, 'learning_rate': 7.118212384678706e-05, 'epoch': 1.28}
{'loss': 0.0757, 'grad_norm': 0.24444185197353363, 'learning_rate': 7.025625256149769e-05, 'epoch': 1.29}
{'loss': 0.0776, 'grad_norm': 0.348590612411499, 'learning_rate': 6.932199538404646e-05, 'epoch': 1.31}
{'loss': 0.0775, 'grad_norm': 0.2950150668621063, 'learning_rate': 6.837973908784654e-05, 'epoch': 1.33}
{'loss': 0.0768, 'grad_norm': 0.3908928632736206, 'learning_rate': 6.742987375786876e-05, 'epoch': 1.35}
{'loss': 0.0703, 'grad_norm': 0.22969897091388702, 'learning_rate': 6.647279262915006e-05, 'epoch': 1.36}
{'loss': 0.0755, 'grad_norm': 0.24255815148353577, 'learning_rate': 6.55088919239982e-05, 'epoch': 1.38}
{'loss': 0.0741, 'grad_norm': 0.2262580394744873, 'learning_rate': 6.453857068795938e-05, 'epoch': 1.4}
{'loss': 0.0796, 'grad_norm': 0.2699412703514099, 'learning_rate': 6.356223062461741e-05, 'epoch': 1.42}
{'loss': 0.0725, 'grad_norm': 0.38178950548171997, 'learning_rate': 6.25802759292922e-05, 'epoch': 1.43}
{'loss': 0.0755, 'grad_norm': 0.40539562702178955, 'learning_rate': 6.159311312170688e-05, 'epoch': 1.45}
{'loss': 0.0738, 'grad_norm': 0.2158227264881134, 'learning_rate': 6.060115087769256e-05, 'epoch': 1.47}
{'loss': 0.0739, 'grad_norm': 0.3278723955154419, 'learning_rate': 5.960479986000045e-05, 'epoch': 1.49}
{'loss': 0.0761, 'grad_norm': 0.23555001616477966, 'learning_rate': 5.860447254829158e-05, 'epoch': 1.5}
{'loss': 0.0732, 'grad_norm': 0.254190057516098, 'learning_rate': 5.760058306837414e-05, 'epoch': 1.52}
{'loss': 0.0754, 'grad_norm': 0.2131957709789276, 'learning_rate': 5.659354702075935e-05, 'epoch': 1.54}
{'loss': 0.0755, 'grad_norm': 0.23241843283176422, 'learning_rate': 5.558378130860707e-05, 'epoch': 1.56}
{'loss': 0.0713, 'grad_norm': 0.26919373869895935, 'learning_rate': 5.4571703965131695e-05, 'epoch': 1.57}
{'loss': 0.0712, 'grad_norm': 0.28385794162750244, 'learning_rate': 5.3557733980540635e-05, 'epoch': 1.59}
{'loss': 0.0691, 'grad_norm': 0.22070147097110748, 'learning_rate': 5.254229112857636e-05, 'epoch': 1.61}
{'loss': 0.0728, 'grad_norm': 0.46119120717048645, 'learning_rate': 5.1525795792734144e-05, 'epoch': 1.63}
{'loss': 0.0717, 'grad_norm': 0.21767249703407288, 'learning_rate': 5.050866879222742e-05, 'epoch': 1.64}
{'loss': 0.0684, 'grad_norm': 0.23991064727306366, 'learning_rate': 4.949133120777259e-05, 'epoch': 1.66}
{'loss': 0.0708, 'grad_norm': 0.2939630150794983, 'learning_rate': 4.8474204207265854e-05, 'epoch': 1.68}
{'loss': 0.0733, 'grad_norm': 0.26071271300315857, 'learning_rate': 4.745770887142366e-05, 'epoch': 1.7}
{'loss': 0.0758, 'grad_norm': 0.20356711745262146, 'learning_rate': 4.644226601945938e-05, 'epoch': 1.71}
{'loss': 0.0755, 'grad_norm': 0.28752267360687256, 'learning_rate': 4.5428296034868324e-05, 'epoch': 1.73}
{'loss': 0.0753, 'grad_norm': 0.29820841550827026, 'learning_rate': 4.441621869139293e-05, 'epoch': 1.75}
{'eval_loss': 0.07409386336803436, 'eval_runtime': 43.1955, 'eval_samples_per_second': 22.294, 'eval_steps_per_second': 2.801, 'epoch': 1.75}
{'loss': 0.0698, 'grad_norm': 0.21687325835227966, 'learning_rate': 4.340645297924064e-05, 'epoch': 1.77}
{'loss': 0.0725, 'grad_norm': 0.2415323406457901, 'learning_rate': 4.2399416931625894e-05, 'epoch': 1.78}
{'loss': 0.0748, 'grad_norm': 0.2155916392803192, 'learning_rate': 4.139552745170843e-05, 'epoch': 1.8}
{'loss': 0.0703, 'grad_norm': 0.22129733860492706, 'learning_rate': 4.0395200139999566e-05, 'epoch': 1.82}
{'loss': 0.0716, 'grad_norm': 0.24352695047855377, 'learning_rate': 3.939884912230746e-05, 'epoch': 1.84}
{'loss': 0.0729, 'grad_norm': 0.3684723973274231, 'learning_rate': 3.840688687829312e-05, 'epoch': 1.85}
{'loss': 0.0733, 'grad_norm': 0.25852376222610474, 'learning_rate': 3.7419724070707806e-05, 'epoch': 1.87}
{'loss': 0.0733, 'grad_norm': 0.24803267419338226, 'learning_rate': 3.64377693753826e-05, 'epoch': 1.89}
{'loss': 0.0722, 'grad_norm': 0.24424618482589722, 'learning_rate': 3.546142931204062e-05, 'epoch': 1.91}
{'loss': 0.0685, 'grad_norm': 0.29537174105644226, 'learning_rate': 3.449110807600182e-05, 'epoch': 1.92}
{'loss': 0.071, 'grad_norm': 0.2558594346046448, 'learning_rate': 3.352720737084994e-05, 'epoch': 1.94}
{'loss': 0.0725, 'grad_norm': 0.20249226689338684, 'learning_rate': 3.257012624213126e-05, 'epoch': 1.96}
{'loss': 0.064, 'grad_norm': 0.41365835070610046, 'learning_rate': 3.162026091215347e-05, 'epoch': 1.98}
{'loss': 0.0704, 'grad_norm': 0.2042560577392578, 'learning_rate': 3.067800461595355e-05, 'epoch': 1.99}
{'loss': 0.0698, 'grad_norm': 0.20796528458595276, 'learning_rate': 2.9743747438502316e-05, 'epoch': 2.01}
{'loss': 0.0644, 'grad_norm': 0.29902905225753784, 'learning_rate': 2.8817876153212958e-05, 'epoch': 2.03}
{'loss': 0.0711, 'grad_norm': 0.20892509818077087, 'learning_rate': 2.7900774061820613e-05, 'epoch': 2.05}
{'loss': 0.069, 'grad_norm': 0.2548845410346985, 'learning_rate': 2.6992820835699133e-05, 'epoch': 2.06}
{'loss': 0.0685, 'grad_norm': 0.1987486183643341, 'learning_rate': 2.609439235868092e-05, 'epoch': 2.08}
{'loss': 0.0619, 'grad_norm': 0.24897199869155884, 'learning_rate': 2.5205860571444563e-05, 'epoch': 2.1}
{'loss': 0.0658, 'grad_norm': 0.2879839837551117, 'learning_rate': 2.4327593317535045e-05, 'epoch': 2.12}
{'loss': 0.067, 'grad_norm': 0.32851848006248474, 'learning_rate': 2.3459954191080065e-05, 'epoch': 2.13}
{'loss': 0.0661, 'grad_norm': 0.39777591824531555, 'learning_rate': 2.2603302386265567e-05, 'epoch': 2.15}
{'loss': 0.0682, 'grad_norm': 0.24693024158477783, 'learning_rate': 2.175799254863294e-05, 'epoch': 2.17}
{'loss': 0.0688, 'grad_norm': 0.403402715921402, 'learning_rate': 2.092437462825908e-05, 'epoch': 2.19}
{'loss': 0.0677, 'grad_norm': 0.2440442591905594, 'learning_rate': 2.010279373488053e-05, 'epoch': 2.2}
{'loss': 0.0697, 'grad_norm': 0.23415102064609528, 'learning_rate': 1.9293589995021337e-05, 'epoch': 2.22}
{'loss': 0.0661, 'grad_norm': 0.2710503935813904, 'learning_rate': 1.8497098411183973e-05, 'epoch': 2.24}
{'loss': 0.067, 'grad_norm': 0.30418142676353455, 'learning_rate': 1.771364872316163e-05, 'epoch': 2.26}
{'loss': 0.0654, 'grad_norm': 0.24616460502147675, 'learning_rate': 1.6943565271529045e-05, 'epoch': 2.27}
{'loss': 0.0684, 'grad_norm': 0.3490281105041504, 'learning_rate': 1.6187166863368713e-05, 'epoch': 2.29}
{'loss': 0.0683, 'grad_norm': 0.3303692936897278, 'learning_rate': 1.544476664028779e-05, 'epoch': 2.31}
{'loss': 0.0647, 'grad_norm': 0.26147621870040894, 'learning_rate': 1.4716671948780513e-05, 'epoch': 2.33}
{'loss': 0.0652, 'grad_norm': 0.22499649226665497, 'learning_rate': 1.40031842129898e-05, 'epoch': 2.34}
{'loss': 0.0627, 'grad_norm': 0.29291996359825134, 'learning_rate': 1.33045988099205e-05, 'epoch': 2.36}
{'loss': 0.0672, 'grad_norm': 0.2038605511188507, 'learning_rate': 1.262120494715624e-05, 'epoch': 2.38}
{'loss': 0.0656, 'grad_norm': 0.26272958517074585, 'learning_rate': 1.195328554313016e-05, 'epoch': 2.4}
{'loss': 0.066, 'grad_norm': 0.3221382796764374, 'learning_rate': 1.130111710999961e-05, 'epoch': 2.41}
{'loss': 0.0624, 'grad_norm': 0.32177722454071045, 'learning_rate': 1.0664969639172672e-05, 'epoch': 2.43}
{'loss': 0.0676, 'grad_norm': 0.3021818995475769, 'learning_rate': 1.0045106489534389e-05, 'epoch': 2.45}
{'loss': 0.0669, 'grad_norm': 0.25223758816719055, 'learning_rate': 9.441784278418686e-06, 'epoch': 2.47}
{'loss': 0.066, 'grad_norm': 0.29470646381378174, 'learning_rate': 8.85525277537132e-06, 'epoch': 2.48}
{'loss': 0.0627, 'grad_norm': 0.42797228693962097, 'learning_rate': 8.285754798747736e-06, 'epoch': 2.5}
{'loss': 0.0659, 'grad_norm': 0.2850504517555237, 'learning_rate': 7.733526115188567e-06, 'epoch': 2.52}
{'loss': 0.068, 'grad_norm': 0.2884679436683655, 'learning_rate': 7.198795342014575e-06, 'epoch': 2.54}
{'loss': 0.0633, 'grad_norm': 0.21456816792488098, 'learning_rate': 6.681783852581252e-06, 'epoch': 2.55}
{'loss': 0.0654, 'grad_norm': 0.2550642192363739, 'learning_rate': 6.182705684632429e-06, 'epoch': 2.57}
{'loss': 0.0638, 'grad_norm': 0.31497493386268616, 'learning_rate': 5.701767451690726e-06, 'epoch': 2.59}
{'loss': 0.0637, 'grad_norm': 0.2880280315876007, 'learning_rate': 5.239168257521549e-06, 'epoch': 2.61}
{'loss': 0.0635, 'grad_norm': 0.27088236808776855, 'learning_rate': 4.7950996137060666e-06, 'epoch': 2.62}
{'loss': 0.0641, 'grad_norm': 0.3014012277126312, 'learning_rate': 4.369745360357258e-06, 'epoch': 2.64}
{'loss': 0.0693, 'grad_norm': 0.3318765163421631, 'learning_rate': 3.963281590011892e-06, 'epoch': 2.66}
{'loss': 0.064, 'grad_norm': 0.24884456396102905, 'learning_rate': 3.575876574729947e-06, 'epoch': 2.68}
{'loss': 0.0646, 'grad_norm': 0.28027772903442383, 'learning_rate': 3.207690696431581e-06, 'epoch': 2.69}
{'loss': 0.0638, 'grad_norm': 0.23885922133922577, 'learning_rate': 2.858876380500608e-06, 'epoch': 2.71}
{'loss': 0.0657, 'grad_norm': 0.2851178050041199, 'learning_rate': 2.5295780326818063e-06, 'epoch': 2.73}
{'loss': 0.0649, 'grad_norm': 0.277808278799057, 'learning_rate': 2.2199319792983896e-06, 'epoch': 2.75}
{'loss': 0.0674, 'grad_norm': 0.2562333345413208, 'learning_rate': 1.9300664108142298e-06, 'epoch': 2.76}
{'loss': 0.0615, 'grad_norm': 0.23543137311935425, 'learning_rate': 1.6601013287642297e-06, 'epoch': 2.78}
{'loss': 0.0632, 'grad_norm': 0.31624266505241394, 'learning_rate': 1.410148496074859e-06, 'epoch': 2.8}
{'loss': 0.0639, 'grad_norm': 0.2809314727783203, 'learning_rate': 1.1803113907953855e-06, 'epoch': 2.82}
{'loss': 0.0655, 'grad_norm': 0.282728374004364, 'learning_rate': 9.706851632589498e-07, 'epoch': 2.83}
{'loss': 0.0702, 'grad_norm': 0.2990603446960449, 'learning_rate': 7.813565966912905e-07, 'epoch': 2.85}
{'loss': 0.0685, 'grad_norm': 0.36267897486686707, 'learning_rate': 6.124040712832846e-07, 'epoch': 2.87}
{'loss': 0.0657, 'grad_norm': 0.31816384196281433, 'learning_rate': 4.638975317423522e-07, 'epoch': 2.89}
{'loss': 0.0643, 'grad_norm': 0.32144126296043396, 'learning_rate': 3.358984583359703e-07, 'epoch': 2.9}
{'loss': 0.064, 'grad_norm': 0.3092522919178009, 'learning_rate': 2.2845984143949895e-07, 'epoch': 2.92}
{'loss': 0.0626, 'grad_norm': 0.22310875356197357, 'learning_rate': 1.4162615959863457e-07, 'epoch': 2.94}
{'loss': 0.066, 'grad_norm': 0.39799514412879944, 'learning_rate': 7.543336111575094e-08, 'epoch': 2.96}
{'loss': 0.07, 'grad_norm': 0.2542976140975952, 'learning_rate': 2.990884916763137e-08, 'epoch': 2.97}
{'loss': 0.067, 'grad_norm': 0.32836732268333435, 'learning_rate': 5.071470460832339e-09, 'epoch': 2.99}
{'train_runtime': 10125.1049, 'train_samples_per_second': 5.418, 'train_steps_per_second': 0.169, 'train_loss': 0.08343932282674563, 'epoch': 3.0}
***** train metrics *****
  epoch                    =        3.0
  total_flos               = 86192852GF
  train_loss               =     0.0834
  train_runtime            = 2:48:45.10
  train_samples_per_second =      5.418
  train_steps_per_second   =      0.169
Figure saved at: saves/mindchat-qwen2-05b/dora/sft/training_loss.png
Figure saved at: saves/mindchat-qwen2-05b/dora/sft/training_eval_loss.png
[WARNING|2025-11-24 19:09:52] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
***** eval metrics *****
  epoch                   =        3.0
  eval_loss               =      0.073
  eval_runtime            = 0:00:43.17
  eval_samples_per_second =     22.303
  eval_steps_per_second   =      2.802
