/share/home/gpu093197/anaconda3/envs/llama-zanewang/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,095 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,095 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,095 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,095 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,095 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,095 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,095 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-11-24 16:14:18,455 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-11-24 16:14:18,459 >> loading configuration file /share/home/gpu093197/.cache/huggingface/models--X-D-Lab--MindChat-Qwen2-0_5B/snapshots/7903453b56c9defe3ee5bd97495802918407ac1c/config.json
[INFO|configuration_utils.py:839] 2025-11-24 16:14:18,463 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": false,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,465 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,465 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,465 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,465 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,465 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,465 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-11-24 16:14:18,465 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-11-24 16:14:18,841 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-11-24 16:14:24,412 >> loading configuration file /share/home/gpu093197/.cache/huggingface/models--X-D-Lab--MindChat-Qwen2-0_5B/snapshots/7903453b56c9defe3ee5bd97495802918407ac1c/config.json
[INFO|configuration_utils.py:839] 2025-11-24 16:14:24,412 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": false,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[WARNING|logging.py:328] 2025-11-24 16:14:26,380 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1169] 2025-11-24 16:14:26,590 >> loading weights file /share/home/gpu093197/.cache/huggingface/models--X-D-Lab--MindChat-Qwen2-0_5B/snapshots/7903453b56c9defe3ee5bd97495802918407ac1c/model.safetensors
[INFO|modeling_utils.py:2341] 2025-11-24 16:14:26,592 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-11-24 16:14:26,594 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "use_cache": false
}

[INFO|configuration_utils.py:939] 2025-11-24 16:14:28,192 >> loading configuration file /share/home/gpu093197/.cache/huggingface/models--X-D-Lab--MindChat-Qwen2-0_5B/snapshots/7903453b56c9defe3ee5bd97495802918407ac1c/generation_config.json
[INFO|configuration_utils.py:986] 2025-11-24 16:14:28,192 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": [
    151645,
    151643
  ]
}

[INFO|dynamic_module_utils.py:423] 2025-11-24 16:14:28,193 >> Could not locate the custom_generate/generate.py inside /share/home/gpu093197/.cache/huggingface/models--X-D-Lab--MindChat-Qwen2-0_5B/snapshots/7903453b56c9defe3ee5bd97495802918407ac1c.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/cli.py", line 31, in <module>
    main()
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/launcher.py", line 157, in launch
    run_exp()
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/train/tuner.py", line 132, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/train/tuner.py", line 93, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/train/sft/workflow.py", line 52, in run_sft
    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/model/loader.py", line 190, in load_model
    model = init_adapter(config, model, model_args, finetuning_args, is_trainable)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/model/adapter.py", line 360, in init_adapter
    model = _setup_lora_tuning(
            ^^^^^^^^^^^^^^^^^^^
  File "/share/home/gpu093197/zanewang/project-llm/LLaMA-Factory-main/src/llamafactory/model/adapter.py", line 240, in _setup_lora_tuning
    raise ValueError("DoRA is not compatible with PTQ-quantized models.")
ValueError: DoRA is not compatible with PTQ-quantized models.
