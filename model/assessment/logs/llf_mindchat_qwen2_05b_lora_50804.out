CUDA_VISIBLE_DEVICES=0
Mon Nov 24 11:16:06 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA RTX A6000               On  | 00000000:AF:00.0 Off |                  Off |
| 30%   32C    P8              24W / 300W |      3MiB / 49140MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
torch: 2.5.1+cu121 cuda: True
[INFO|2025-11-24 11:18:22] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16
[INFO|2025-11-24 11:18:23] llamafactory.data.loader:143 >> Loading dataset sft_student_mental.jsonl...
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14880, 100345, 105051, 43815, 102086, 116743, 105151, 90395, 91680, 66017, 5370, 5122, 450, 4011, 8274, 7, 15, 12, 18, 701, 18056, 8274, 7, 15, 12, 18, 701, 5214, 10933, 1445, 603, 91, 27051, 292, 25880, 91, 721, 1523, 2178, 91, 55151, 763, 701, 5458, 16031, 673, 10405, 7, 15, 12, 24, 8, 8997, 872, 25, 88940, 225, 21887, 100703, 99235, 3837, 108386, 3837, 35946, 104044, 99879, 35946, 104858, 87267, 113783, 34187, 3837, 43288, 104029, 99491, 104573, 1773, 35946, 114606, 94432, 872, 25, 49434, 239, 33108, 35946, 104858, 20412, 99562, 101181, 3837, 97639, 104080, 99461, 109745, 34187, 3837, 104133, 20, 104252, 99657, 1773, 100543, 24562, 3837, 111003, 42411, 87267, 113783, 34187, 8997, 872, 25, 49434, 239, 106546, 100648, 58405, 69249, 33108, 46944, 57750, 9370, 101194, 105610, 33108, 87805, 1773, 42411, 104136, 36587, 99650, 100009, 99614, 3837, 105984, 104083, 100698, 8997, 872, 25, 49434, 239, 79478, 99601, 106822, 105919, 104432, 1773, 35946, 104014, 106118, 85336, 100111, 100648, 58405, 3837, 97639, 103977, 99519, 111553, 116784, 8997, 872, 25, 49434, 239, 109623, 35946, 107999, 101901, 111852, 42411, 3837, 70361, 100641, 102497, 1773, 110146, 104359, 104899, 103952, 104617, 33108, 101064, 8997, 872, 25, 49434, 239, 99880, 103952, 104617, 100006, 101954, 3837, 101064, 117809, 1773, 101553, 99880, 99519, 111553, 99564, 26939, 103952, 100145, 8997, 872, 25, 49434, 239, 101045, 99165, 104409, 1773, 102389, 3837, 106922, 99242, 42411, 3837, 104359, 104899, 103952, 104617, 24968, 104702, 3837, 35946, 104083, 102827, 111553, 3837, 70361, 100641, 102497, 8997, 872, 25, 18137, 224, 96, 110693, 99730, 109031, 94432, 872, 25, 4891, 98, 121, 9370, 3837, 105351, 101066, 85336, 104482, 1773, 102570, 103929, 100364, 1773, 151645, 198, 151644, 77091, 198, 4913, 450, 4011, 8274, 788, 220, 16, 11, 330, 276, 16308, 8274, 788, 220, 17, 11, 330, 80943, 10933, 788, 330, 6697, 497, 330, 12038, 16031, 673, 10405, 788, 220, 21, 92, 151645, 198]
inputs:
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
请根据对话内容评估四项标签，并只输出JSON：depression_level(0-3), anxiety_level(0-3), risk_flag(none|suicidal|self_harm|violence), student_distress_score(0-9)。
user: 心理咨询师，你好，我最近发现我老公可能出轨了，这让我非常痛苦。我该怎么办？
user: 我和我老公是大学同学，我们在一起已经十几年了，有一个5岁的孩子。半年前，我发现他可能出轨了。
user: 我发现了他的手机里和一个女的有很多短信和电话。他解释说他们只是朋友，但我很难相信。
user: 我们现在相处得很紧张。我总是忍不住去查看他的手机，我们也会因为这件事情争吵。
user: 我想知道我该如何才能原谅他，重新建立信任。我真的不想失去我们的婚姻和家庭。
user: 我希望我们的婚姻能够幸福，家庭和睦。我不希望因为这件事情影响到我们的关系。
user: 我其实很矛盾。一方面，我很爱他，不想失去我们的婚姻；另一方面，我很难忘记这件事情，重新建立信任。
user: 那我们现在应该怎么做？
user: 好的，我会努力去尝试。谢谢你的帮助。<|im_end|>
<|im_start|>assistant
{"depression_level": 1, "anxiety_level": 2, "risk_flag": "none", "student_distress_score": 6}<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 450, 4011, 8274, 788, 220, 16, 11, 330, 276, 16308, 8274, 788, 220, 17, 11, 330, 80943, 10933, 788, 330, 6697, 497, 330, 12038, 16031, 673, 10405, 788, 220, 21, 92, 151645, 198]
labels:
{"depression_level": 1, "anxiety_level": 2, "risk_flag": "none", "student_distress_score": 6}<|im_end|>

[INFO|2025-11-24 11:18:45] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|2025-11-24 11:18:45] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-11-24 11:19:22] llamafactory.model.model_utils.checkpointing:143 >> Upcasting layernorm weights in float32.
[INFO|2025-11-24 11:19:22] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-11-24 11:19:22] llamafactory.model.model_utils.checkpointing:143 >> Upcasting lm_head outputs in float32.
[INFO|2025-11-24 11:19:22] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-11-24 11:19:22] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-11-24 11:19:22] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-11-24 11:19:22] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,v_proj,o_proj,down_proj,k_proj,up_proj,q_proj
[INFO|2025-11-24 11:19:22] llamafactory.model.loader:143 >> trainable params: 7,569,408 || all params: 471,557,120 || trainable%: 1.6052
{'loss': 0.5468, 'grad_norm': 2.7262532711029053, 'learning_rate': 5.232558139534884e-06, 'epoch': 0.02}
{'loss': 0.4255, 'grad_norm': 1.7796484231948853, 'learning_rate': 1.1046511627906977e-05, 'epoch': 0.03}
{'loss': 0.2429, 'grad_norm': 0.4850206971168518, 'learning_rate': 1.686046511627907e-05, 'epoch': 0.05}
{'loss': 0.1692, 'grad_norm': 0.43001922965049744, 'learning_rate': 2.2674418604651163e-05, 'epoch': 0.07}
{'loss': 0.1513, 'grad_norm': 0.3702583312988281, 'learning_rate': 2.848837209302326e-05, 'epoch': 0.09}
{'loss': 0.1427, 'grad_norm': 0.3907802402973175, 'learning_rate': 3.430232558139535e-05, 'epoch': 0.1}
{'loss': 0.1392, 'grad_norm': 0.43508535623550415, 'learning_rate': 4.0116279069767444e-05, 'epoch': 0.12}
{'loss': 0.1258, 'grad_norm': 0.4638979136943817, 'learning_rate': 4.593023255813954e-05, 'epoch': 0.14}
{'loss': 0.121, 'grad_norm': 0.7222078442573547, 'learning_rate': 5.1744186046511636e-05, 'epoch': 0.16}
{'loss': 0.1269, 'grad_norm': 0.9610591530799866, 'learning_rate': 5.755813953488373e-05, 'epoch': 0.17}
{'loss': 0.1148, 'grad_norm': 0.6405833959579468, 'learning_rate': 6.337209302325582e-05, 'epoch': 0.19}
{'loss': 0.1159, 'grad_norm': 0.7796623110771179, 'learning_rate': 6.918604651162791e-05, 'epoch': 0.21}
{'loss': 0.1131, 'grad_norm': 0.5455369353294373, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.23}
{'loss': 0.1105, 'grad_norm': 0.40003031492233276, 'learning_rate': 8.081395348837209e-05, 'epoch': 0.24}
{'loss': 0.1084, 'grad_norm': 0.5200830101966858, 'learning_rate': 8.662790697674419e-05, 'epoch': 0.26}
{'loss': 0.11, 'grad_norm': 0.6509326100349426, 'learning_rate': 9.244186046511628e-05, 'epoch': 0.28}
{'loss': 0.107, 'grad_norm': 0.4516127407550812, 'learning_rate': 9.825581395348838e-05, 'epoch': 0.3}
{'loss': 0.1042, 'grad_norm': 0.6017825603485107, 'learning_rate': 9.999492852953918e-05, 'epoch': 0.31}
{'loss': 0.1018, 'grad_norm': 0.2938265800476074, 'learning_rate': 9.997009115083239e-05, 'epoch': 0.33}
{'loss': 0.0927, 'grad_norm': 0.4176541268825531, 'learning_rate': 9.992456663888425e-05, 'epoch': 0.35}
{'loss': 0.0923, 'grad_norm': 0.4947246313095093, 'learning_rate': 9.985837384040138e-05, 'epoch': 0.37}
{'loss': 0.0926, 'grad_norm': 0.2539445161819458, 'learning_rate': 9.97715401585605e-05, 'epoch': 0.38}
{'loss': 0.0957, 'grad_norm': 0.48263412714004517, 'learning_rate': 9.966410154166403e-05, 'epoch': 0.4}
{'loss': 0.092, 'grad_norm': 0.26502326130867004, 'learning_rate': 9.953610246825765e-05, 'epoch': 0.42}
{'loss': 0.0965, 'grad_norm': 0.33158156275749207, 'learning_rate': 9.938759592871671e-05, 'epoch': 0.44}
{'loss': 0.0925, 'grad_norm': 0.3395930528640747, 'learning_rate': 9.921864340330871e-05, 'epoch': 0.45}
{'loss': 0.0921, 'grad_norm': 0.3837187886238098, 'learning_rate': 9.902931483674105e-05, 'epoch': 0.47}
{'loss': 0.0895, 'grad_norm': 0.6402357816696167, 'learning_rate': 9.881968860920463e-05, 'epoch': 0.49}
{'loss': 0.0864, 'grad_norm': 0.32064560055732727, 'learning_rate': 9.858985150392515e-05, 'epoch': 0.51}
{'loss': 0.0911, 'grad_norm': 0.17896711826324463, 'learning_rate': 9.833989867123577e-05, 'epoch': 0.52}
{'loss': 0.0886, 'grad_norm': 0.3827873468399048, 'learning_rate': 9.806993358918578e-05, 'epoch': 0.54}
{'loss': 0.0887, 'grad_norm': 0.43563368916511536, 'learning_rate': 9.778006802070162e-05, 'epoch': 0.56}
{'loss': 0.0868, 'grad_norm': 0.312323659658432, 'learning_rate': 9.74704219673182e-05, 'epoch': 0.58}
{'loss': 0.0886, 'grad_norm': 0.2507535219192505, 'learning_rate': 9.71411236194994e-05, 'epoch': 0.59}
{'loss': 0.0836, 'grad_norm': 0.28908446431159973, 'learning_rate': 9.679230930356841e-05, 'epoch': 0.61}
{'loss': 0.0828, 'grad_norm': 0.3464077413082123, 'learning_rate': 9.642412342527006e-05, 'epoch': 0.63}
{'loss': 0.0856, 'grad_norm': 0.46217310428619385, 'learning_rate': 9.603671840998811e-05, 'epoch': 0.65}
{'loss': 0.0918, 'grad_norm': 0.2935827076435089, 'learning_rate': 9.563025463964275e-05, 'epoch': 0.66}
{'loss': 0.0915, 'grad_norm': 0.25844982266426086, 'learning_rate': 9.520490038629394e-05, 'epoch': 0.68}
{'loss': 0.0809, 'grad_norm': 0.5009990930557251, 'learning_rate': 9.476083174247845e-05, 'epoch': 0.7}
{'loss': 0.0774, 'grad_norm': 0.3212243616580963, 'learning_rate': 9.429823254830929e-05, 'epoch': 0.72}
{'loss': 0.0775, 'grad_norm': 0.3229734003543854, 'learning_rate': 9.381729431536758e-05, 'epoch': 0.73}
{'loss': 0.0789, 'grad_norm': 0.27736178040504456, 'learning_rate': 9.331821614741876e-05, 'epoch': 0.75}
{'loss': 0.0821, 'grad_norm': 0.2735542356967926, 'learning_rate': 9.280120465798543e-05, 'epoch': 0.77}
{'loss': 0.0885, 'grad_norm': 0.2023923695087433, 'learning_rate': 9.226647388481144e-05, 'epoch': 0.79}
{'loss': 0.0833, 'grad_norm': 0.30099934339523315, 'learning_rate': 9.171424520125227e-05, 'epoch': 0.8}
{'loss': 0.078, 'grad_norm': 0.3210887908935547, 'learning_rate': 9.11447472246287e-05, 'epoch': 0.82}
{'loss': 0.0835, 'grad_norm': 0.2904280126094818, 'learning_rate': 9.055821572158133e-05, 'epoch': 0.84}
{'loss': 0.0808, 'grad_norm': 0.2806582450866699, 'learning_rate': 8.995489351046562e-05, 'epoch': 0.86}
{'loss': 0.0844, 'grad_norm': 0.4317253530025482, 'learning_rate': 8.933503036082733e-05, 'epoch': 0.87}
{'loss': 0.0813, 'grad_norm': 0.2887954115867615, 'learning_rate': 8.86988828900004e-05, 'epoch': 0.89}
{'loss': 0.0786, 'grad_norm': 0.2946949601173401, 'learning_rate': 8.804671445686985e-05, 'epoch': 0.91}
{'loss': 0.0785, 'grad_norm': 0.2605745494365692, 'learning_rate': 8.737879505284378e-05, 'epoch': 0.93}
{'loss': 0.0842, 'grad_norm': 0.4172660708427429, 'learning_rate': 8.66954011900795e-05, 'epoch': 0.94}
{'loss': 0.0818, 'grad_norm': 0.32533082365989685, 'learning_rate': 8.59968157870102e-05, 'epoch': 0.96}
{'loss': 0.0826, 'grad_norm': 0.22251231968402863, 'learning_rate': 8.52833280512195e-05, 'epoch': 0.98}
{'loss': 0.0797, 'grad_norm': 0.34368574619293213, 'learning_rate': 8.455523335971223e-05, 'epoch': 1.0}
{'loss': 0.0791, 'grad_norm': 0.3818103075027466, 'learning_rate': 8.381283313663129e-05, 'epoch': 1.01}
{'loss': 0.0761, 'grad_norm': 0.24170000851154327, 'learning_rate': 8.305643472847095e-05, 'epoch': 1.03}
{'loss': 0.0772, 'grad_norm': 0.24019356071949005, 'learning_rate': 8.228635127683836e-05, 'epoch': 1.05}
{'loss': 0.0753, 'grad_norm': 0.27143627405166626, 'learning_rate': 8.150290158881604e-05, 'epoch': 1.07}
{'loss': 0.078, 'grad_norm': 0.28509581089019775, 'learning_rate': 8.07064100049787e-05, 'epoch': 1.08}
{'loss': 0.0773, 'grad_norm': 0.3574378192424774, 'learning_rate': 7.989720626511947e-05, 'epoch': 1.1}
{'loss': 0.0775, 'grad_norm': 0.219040185213089, 'learning_rate': 7.907562537174091e-05, 'epoch': 1.12}
{'loss': 0.0759, 'grad_norm': 0.2658654451370239, 'learning_rate': 7.824200745136707e-05, 'epoch': 1.14}
{'loss': 0.0721, 'grad_norm': 0.363633394241333, 'learning_rate': 7.739669761373444e-05, 'epoch': 1.15}
{'loss': 0.0762, 'grad_norm': 0.24661904573440552, 'learning_rate': 7.654004580891997e-05, 'epoch': 1.17}
{'loss': 0.0788, 'grad_norm': 0.32164299488067627, 'learning_rate': 7.567240668246496e-05, 'epoch': 1.19}
{'loss': 0.0788, 'grad_norm': 0.3914615213871002, 'learning_rate': 7.479413942855544e-05, 'epoch': 1.21}
{'loss': 0.0776, 'grad_norm': 0.2887920141220093, 'learning_rate': 7.390560764131909e-05, 'epoch': 1.22}
{'loss': 0.0735, 'grad_norm': 0.2290252447128296, 'learning_rate': 7.300717916430088e-05, 'epoch': 1.24}
{'loss': 0.0771, 'grad_norm': 0.4773871898651123, 'learning_rate': 7.209922593817941e-05, 'epoch': 1.26}
{'loss': 0.0693, 'grad_norm': 0.24581705033779144, 'learning_rate': 7.118212384678706e-05, 'epoch': 1.28}
{'loss': 0.076, 'grad_norm': 0.25781944394111633, 'learning_rate': 7.025625256149769e-05, 'epoch': 1.29}
{'loss': 0.0787, 'grad_norm': 0.3180212080478668, 'learning_rate': 6.932199538404646e-05, 'epoch': 1.31}
{'loss': 0.079, 'grad_norm': 0.31540337204933167, 'learning_rate': 6.837973908784654e-05, 'epoch': 1.33}
{'loss': 0.0784, 'grad_norm': 0.38016578555107117, 'learning_rate': 6.742987375786876e-05, 'epoch': 1.35}
{'loss': 0.071, 'grad_norm': 0.2275421917438507, 'learning_rate': 6.647279262915006e-05, 'epoch': 1.36}
{'loss': 0.0766, 'grad_norm': 0.2577647566795349, 'learning_rate': 6.55088919239982e-05, 'epoch': 1.38}
{'loss': 0.0752, 'grad_norm': 0.23343725502490997, 'learning_rate': 6.453857068795938e-05, 'epoch': 1.4}
{'loss': 0.0807, 'grad_norm': 0.2275245636701584, 'learning_rate': 6.356223062461741e-05, 'epoch': 1.42}
{'loss': 0.0733, 'grad_norm': 0.40886491537094116, 'learning_rate': 6.25802759292922e-05, 'epoch': 1.43}
{'loss': 0.076, 'grad_norm': 0.48014575242996216, 'learning_rate': 6.159311312170688e-05, 'epoch': 1.45}
{'loss': 0.0739, 'grad_norm': 0.20606844127178192, 'learning_rate': 6.060115087769256e-05, 'epoch': 1.47}
{'loss': 0.0755, 'grad_norm': 0.30351319909095764, 'learning_rate': 5.960479986000045e-05, 'epoch': 1.49}
{'loss': 0.0758, 'grad_norm': 0.2488507479429245, 'learning_rate': 5.860447254829158e-05, 'epoch': 1.5}
{'loss': 0.0738, 'grad_norm': 0.25835880637168884, 'learning_rate': 5.760058306837414e-05, 'epoch': 1.52}
{'loss': 0.0749, 'grad_norm': 0.2344609647989273, 'learning_rate': 5.659354702075935e-05, 'epoch': 1.54}
{'loss': 0.0758, 'grad_norm': 0.23222650587558746, 'learning_rate': 5.558378130860707e-05, 'epoch': 1.56}
{'loss': 0.0722, 'grad_norm': 0.29220515489578247, 'learning_rate': 5.4571703965131695e-05, 'epoch': 1.57}
{'loss': 0.0722, 'grad_norm': 0.27246201038360596, 'learning_rate': 5.3557733980540635e-05, 'epoch': 1.59}
{'loss': 0.0701, 'grad_norm': 0.2198297679424286, 'learning_rate': 5.254229112857636e-05, 'epoch': 1.61}
{'loss': 0.073, 'grad_norm': 0.4187157154083252, 'learning_rate': 5.1525795792734144e-05, 'epoch': 1.63}
{'loss': 0.0729, 'grad_norm': 0.22737601399421692, 'learning_rate': 5.050866879222742e-05, 'epoch': 1.64}
{'loss': 0.0702, 'grad_norm': 0.23455514013767242, 'learning_rate': 4.949133120777259e-05, 'epoch': 1.66}
{'loss': 0.0715, 'grad_norm': 0.30913516879081726, 'learning_rate': 4.8474204207265854e-05, 'epoch': 1.68}
{'loss': 0.0734, 'grad_norm': 0.26175040006637573, 'learning_rate': 4.745770887142366e-05, 'epoch': 1.7}
{'loss': 0.0757, 'grad_norm': 0.20031005144119263, 'learning_rate': 4.644226601945938e-05, 'epoch': 1.71}
{'loss': 0.0756, 'grad_norm': 0.29947155714035034, 'learning_rate': 4.5428296034868324e-05, 'epoch': 1.73}
{'loss': 0.0753, 'grad_norm': 0.3069278299808502, 'learning_rate': 4.441621869139293e-05, 'epoch': 1.75}
{'eval_loss': 0.07433265447616577, 'eval_runtime': 35.902, 'eval_samples_per_second': 26.823, 'eval_steps_per_second': 3.37, 'epoch': 1.75}
{'loss': 0.0697, 'grad_norm': 0.23082099854946136, 'learning_rate': 4.340645297924064e-05, 'epoch': 1.77}
{'loss': 0.0732, 'grad_norm': 0.261648952960968, 'learning_rate': 4.2399416931625894e-05, 'epoch': 1.78}
{'loss': 0.0751, 'grad_norm': 0.25459596514701843, 'learning_rate': 4.139552745170843e-05, 'epoch': 1.8}
{'loss': 0.0705, 'grad_norm': 0.2406006008386612, 'learning_rate': 4.0395200139999566e-05, 'epoch': 1.82}
{'loss': 0.0709, 'grad_norm': 0.2479419708251953, 'learning_rate': 3.939884912230746e-05, 'epoch': 1.84}
{'loss': 0.0723, 'grad_norm': 0.35237792134284973, 'learning_rate': 3.840688687829312e-05, 'epoch': 1.85}
{'loss': 0.0729, 'grad_norm': 0.26565656065940857, 'learning_rate': 3.7419724070707806e-05, 'epoch': 1.87}
{'loss': 0.0743, 'grad_norm': 0.2366572916507721, 'learning_rate': 3.64377693753826e-05, 'epoch': 1.89}
{'loss': 0.0735, 'grad_norm': 0.25442230701446533, 'learning_rate': 3.546142931204062e-05, 'epoch': 1.91}
{'loss': 0.0693, 'grad_norm': 0.25998958945274353, 'learning_rate': 3.449110807600182e-05, 'epoch': 1.92}
{'loss': 0.0716, 'grad_norm': 0.26790985465049744, 'learning_rate': 3.352720737084994e-05, 'epoch': 1.94}
{'loss': 0.073, 'grad_norm': 0.2054957002401352, 'learning_rate': 3.257012624213126e-05, 'epoch': 1.96}
{'loss': 0.0651, 'grad_norm': 0.4390679597854614, 'learning_rate': 3.162026091215347e-05, 'epoch': 1.98}
{'loss': 0.0711, 'grad_norm': 0.2131059765815735, 'learning_rate': 3.067800461595355e-05, 'epoch': 1.99}
{'loss': 0.0712, 'grad_norm': 0.2149074673652649, 'learning_rate': 2.9743747438502316e-05, 'epoch': 2.01}
{'loss': 0.0658, 'grad_norm': 0.29382169246673584, 'learning_rate': 2.8817876153212958e-05, 'epoch': 2.03}
{'loss': 0.0712, 'grad_norm': 0.20028123259544373, 'learning_rate': 2.7900774061820613e-05, 'epoch': 2.05}
{'loss': 0.0699, 'grad_norm': 0.2501092255115509, 'learning_rate': 2.6992820835699133e-05, 'epoch': 2.06}
{'loss': 0.069, 'grad_norm': 0.21082265675067902, 'learning_rate': 2.609439235868092e-05, 'epoch': 2.08}
{'loss': 0.0621, 'grad_norm': 0.2408638596534729, 'learning_rate': 2.5205860571444563e-05, 'epoch': 2.1}
{'loss': 0.0667, 'grad_norm': 0.2946993410587311, 'learning_rate': 2.4327593317535045e-05, 'epoch': 2.12}
{'loss': 0.0664, 'grad_norm': 0.3420477509498596, 'learning_rate': 2.3459954191080065e-05, 'epoch': 2.13}
{'loss': 0.0676, 'grad_norm': 0.38461872935295105, 'learning_rate': 2.2603302386265567e-05, 'epoch': 2.15}
{'loss': 0.0694, 'grad_norm': 0.257321298122406, 'learning_rate': 2.175799254863294e-05, 'epoch': 2.17}
{'loss': 0.0694, 'grad_norm': 0.3844687044620514, 'learning_rate': 2.092437462825908e-05, 'epoch': 2.19}
{'loss': 0.068, 'grad_norm': 0.22922861576080322, 'learning_rate': 2.010279373488053e-05, 'epoch': 2.2}
{'loss': 0.0705, 'grad_norm': 0.23497329652309418, 'learning_rate': 1.9293589995021337e-05, 'epoch': 2.22}
{'loss': 0.0669, 'grad_norm': 0.2932192385196686, 'learning_rate': 1.8497098411183973e-05, 'epoch': 2.24}
{'loss': 0.0686, 'grad_norm': 0.2852768301963806, 'learning_rate': 1.771364872316163e-05, 'epoch': 2.26}
{'loss': 0.0667, 'grad_norm': 0.2550954520702362, 'learning_rate': 1.6943565271529045e-05, 'epoch': 2.27}
{'loss': 0.0695, 'grad_norm': 0.34992000460624695, 'learning_rate': 1.6187166863368713e-05, 'epoch': 2.29}
{'loss': 0.0693, 'grad_norm': 0.3591824173927307, 'learning_rate': 1.544476664028779e-05, 'epoch': 2.31}
{'loss': 0.0648, 'grad_norm': 0.2637813985347748, 'learning_rate': 1.4716671948780513e-05, 'epoch': 2.33}
{'loss': 0.066, 'grad_norm': 0.22962366044521332, 'learning_rate': 1.40031842129898e-05, 'epoch': 2.34}
{'loss': 0.0631, 'grad_norm': 0.2959223687648773, 'learning_rate': 1.33045988099205e-05, 'epoch': 2.36}
{'loss': 0.0675, 'grad_norm': 0.20330236852169037, 'learning_rate': 1.262120494715624e-05, 'epoch': 2.38}
{'loss': 0.0663, 'grad_norm': 0.27373817563056946, 'learning_rate': 1.195328554313016e-05, 'epoch': 2.4}
{'loss': 0.0667, 'grad_norm': 0.3489779531955719, 'learning_rate': 1.130111710999961e-05, 'epoch': 2.41}
{'loss': 0.0622, 'grad_norm': 0.30409348011016846, 'learning_rate': 1.0664969639172672e-05, 'epoch': 2.43}
{'loss': 0.0684, 'grad_norm': 0.3190676271915436, 'learning_rate': 1.0045106489534389e-05, 'epoch': 2.45}
{'loss': 0.0671, 'grad_norm': 0.23547764122486115, 'learning_rate': 9.441784278418686e-06, 'epoch': 2.47}
{'loss': 0.0673, 'grad_norm': 0.3248685300350189, 'learning_rate': 8.85525277537132e-06, 'epoch': 2.48}
{'loss': 0.0634, 'grad_norm': 0.3447372019290924, 'learning_rate': 8.285754798747736e-06, 'epoch': 2.5}
{'loss': 0.0664, 'grad_norm': 0.29736822843551636, 'learning_rate': 7.733526115188567e-06, 'epoch': 2.52}
{'loss': 0.0684, 'grad_norm': 0.2758426368236542, 'learning_rate': 7.198795342014575e-06, 'epoch': 2.54}
{'loss': 0.0644, 'grad_norm': 0.24664311110973358, 'learning_rate': 6.681783852581252e-06, 'epoch': 2.55}
{'loss': 0.0651, 'grad_norm': 0.25151732563972473, 'learning_rate': 6.182705684632429e-06, 'epoch': 2.57}
{'loss': 0.0641, 'grad_norm': 0.3254878520965576, 'learning_rate': 5.701767451690726e-06, 'epoch': 2.59}
{'loss': 0.064, 'grad_norm': 0.2798231244087219, 'learning_rate': 5.239168257521549e-06, 'epoch': 2.61}
{'loss': 0.0642, 'grad_norm': 0.29080405831336975, 'learning_rate': 4.7950996137060666e-06, 'epoch': 2.62}
{'loss': 0.065, 'grad_norm': 0.3001261055469513, 'learning_rate': 4.369745360357258e-06, 'epoch': 2.64}
{'loss': 0.0705, 'grad_norm': 0.33026012778282166, 'learning_rate': 3.963281590011892e-06, 'epoch': 2.66}
{'loss': 0.0644, 'grad_norm': 0.24986456334590912, 'learning_rate': 3.575876574729947e-06, 'epoch': 2.68}
{'loss': 0.064, 'grad_norm': 0.2775067687034607, 'learning_rate': 3.207690696431581e-06, 'epoch': 2.69}
{'loss': 0.0649, 'grad_norm': 0.2663014233112335, 'learning_rate': 2.858876380500608e-06, 'epoch': 2.71}
{'loss': 0.0668, 'grad_norm': 0.30557385087013245, 'learning_rate': 2.5295780326818063e-06, 'epoch': 2.73}
{'loss': 0.0655, 'grad_norm': 0.25767281651496887, 'learning_rate': 2.2199319792983896e-06, 'epoch': 2.75}
{'loss': 0.0677, 'grad_norm': 0.24847812950611115, 'learning_rate': 1.9300664108142298e-06, 'epoch': 2.76}
{'loss': 0.0631, 'grad_norm': 0.258283793926239, 'learning_rate': 1.6601013287642297e-06, 'epoch': 2.78}
{'loss': 0.064, 'grad_norm': 0.3107544779777527, 'learning_rate': 1.410148496074859e-06, 'epoch': 2.8}
{'loss': 0.0655, 'grad_norm': 0.30439576506614685, 'learning_rate': 1.1803113907953855e-06, 'epoch': 2.82}
{'loss': 0.0651, 'grad_norm': 0.2965574264526367, 'learning_rate': 9.706851632589498e-07, 'epoch': 2.83}
{'loss': 0.0712, 'grad_norm': 0.29829102754592896, 'learning_rate': 7.813565966912905e-07, 'epoch': 2.85}
{'loss': 0.0685, 'grad_norm': 0.3737465739250183, 'learning_rate': 6.124040712832846e-07, 'epoch': 2.87}
{'loss': 0.0666, 'grad_norm': 0.2790295481681824, 'learning_rate': 4.638975317423522e-07, 'epoch': 2.89}
{'loss': 0.064, 'grad_norm': 0.32066580653190613, 'learning_rate': 3.358984583359703e-07, 'epoch': 2.9}
{'loss': 0.0659, 'grad_norm': 0.3083692789077759, 'learning_rate': 2.2845984143949895e-07, 'epoch': 2.92}
{'loss': 0.0632, 'grad_norm': 0.22819280624389648, 'learning_rate': 1.4162615959863457e-07, 'epoch': 2.94}
{'loss': 0.066, 'grad_norm': 0.4019409716129303, 'learning_rate': 7.543336111575094e-08, 'epoch': 2.96}
{'loss': 0.0705, 'grad_norm': 0.2499573975801468, 'learning_rate': 2.990884916763137e-08, 'epoch': 2.97}
{'loss': 0.0669, 'grad_norm': 0.3189394176006317, 'learning_rate': 5.071470460832339e-09, 'epoch': 2.99}
{'train_runtime': 6992.2554, 'train_samples_per_second': 7.846, 'train_steps_per_second': 0.245, 'train_loss': 0.08458727698920768, 'epoch': 3.0}
***** train metrics *****
  epoch                    =        3.0
  total_flos               = 86122518GF
  train_loss               =     0.0846
  train_runtime            = 1:56:32.25
  train_samples_per_second =      7.846
  train_steps_per_second   =      0.245
Figure saved at: saves/mindchat-qwen2-05b/lora/sft/training_loss.png
Figure saved at: saves/mindchat-qwen2-05b/lora/sft/training_eval_loss.png
[WARNING|2025-11-24 13:15:56] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
***** eval metrics *****
  epoch                   =        3.0
  eval_loss               =     0.0733
  eval_runtime            = 0:00:35.82
  eval_samples_per_second =     26.881
  eval_steps_per_second   =      3.378
